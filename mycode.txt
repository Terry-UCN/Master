import os
import shutil
import random
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, datasets, models
from torch.utils.data import DataLoader
from tqdm import tqdm

# === 1. è³‡æ–™åˆ‡åˆ† ===
def split_dataset(source_root, target_root, classes, test_ratio=0.2):
    for split in ['train', 'test']:
        for cls in classes:
            out_path = os.path.join(target_root, split, cls)
            os.makedirs(out_path, exist_ok=True)

    for cls in classes:
        src_folder = os.path.join(source_root, cls)
        all_images = [f for f in os.listdir(src_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        random.shuffle(all_images)

        n_total = len(all_images)
        n_test = int(n_total * test_ratio)

        test_images = all_images[:n_test]
        train_images = all_images[n_test:]

        for img in test_images:
            shutil.copy2(os.path.join(src_folder, img), os.path.join(target_root, 'test', cls, img))
        for img in train_images:
            shutil.copy2(os.path.join(src_folder, img), os.path.join(target_root, 'train', cls, img))

    print("âœ… è³‡æ–™åˆ‡åˆ†å®Œæˆã€‚")


# === 2. ä¸»ç¨‹å¼ ===
def main():
    # === è³‡æ–™èˆ‡é¡åˆ¥ ===
    source_root = r"C:\Users\CC\Desktop\tesy\data\Garbage classification\Garbage classification"
    target_root = r"C:\Users\CC\Desktop\tesy\data\Garbage classification split"
    classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic']
    num_classes = len(classes)

    # === è³‡æ–™åˆ‡åˆ†ï¼ˆç¬¬ä¸€æ¬¡åŸ·è¡Œæ‰åŸ·è¡Œï¼‰===
    if not os.path.exists(os.path.join(target_root, 'train')):
        split_dataset(source_root, target_root, classes)

    # === åœ–åƒè½‰æ›ï¼ˆå«è³‡æ–™å¢å¼·ï¼‰ ===
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # EfficientNet é è¨“ç·´ mean/std
    ])
    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # === Dataset & DataLoader ===
    train_dataset = datasets.ImageFolder(root=os.path.join(target_root, 'train'), transform=transform_train)
    test_dataset = datasets.ImageFolder(root=os.path.join(target_root, 'test'), transform=transform_test)
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)

    # === EfficientNet æ¨¡å‹ ===
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = models.efficientnet_b0(pretrained=True)

    # æ›¿æ›æœ€å¾Œåˆ†é¡å±¤
    in_features = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(in_features, num_classes)
    model = model.to(device)

    # === æå¤±èˆ‡å„ªåŒ–å™¨ ===
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    # === è¨“ç·´æ¨¡å‹ ===
    # === è¨“ç·´æ¨¡å‹ ===
    best_acc = 0.0
    epochs = 10
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_acc = 100 * correct / total
        print(f"Train Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}%")

        # === é©—è­‰æ¨¡å‹ (æ¯å›åˆå¾Œé©—è­‰ä¸€æ¬¡ä¸¦å„²å­˜æœ€ä½³æ¨¡å‹) ===
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        val_acc = 100 * correct / total
        print(f"âœ… Validation Accuracy: {val_acc:.2f}%")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "best_trash_model.pth")
            print("ğŸ’¾ Best model saved!")

    print(f"\nğŸ‰ æœ€ä½³æº–ç¢ºç‡: {best_acc:.2f}%")


if __name__ == "__main__":
    main()




import cv2
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image

# === è¨­å®šè£ç½® ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === å»ºç«‹èˆ‡è¼‰å…¥æ¨¡å‹ ===
model = models.efficientnet_b0(pretrained=False)
model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 5)
model.load_state_dict(torch.load("best_trash_model.pth", map_location=device))
model = model.to(device)
model.eval()

# === é¡åˆ¥åç¨± ===
classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic']

# === é è™•ç†æµç¨‹ï¼ˆèˆ‡è¨“ç·´ä¸€è‡´ï¼‰===
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# === é–‹å•Ÿ webcam ===
cap = cv2.VideoCapture("CHANGE TO YOUR IP CAM ACCESS/video")
  # æ›¿æ›ç‚ºä½ çš„æ”å½±æ©Ÿ URL

print("ğŸ“· é–‹å§‹å³æ™‚è¾¨è­˜ï¼ˆæŒ‰ä¸‹ q éµå¯é€€å‡ºï¼‰")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # æ“·å–ç•«é¢ä¸­å¤®å€åŸŸä½œç‚ºç›®æ¨™æ¡†ï¼ˆå¯ä¾å¯¦éš›èª¿æ•´ï¼‰
    h, w, _ = frame.shape
    crop_size = 224
    center_x, center_y = w // 2, h // 2
    x1, y1 = center_x - crop_size // 2, center_y - crop_size // 2
    x2, y2 = center_x + crop_size // 2, center_y + crop_size // 2
    cropped = frame[y1:y2, x1:x2]

    # é è™•ç†
    image = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))
    input_tensor = transform(image).unsqueeze(0).to(device)

    # æ¨è«–
    with torch.no_grad():
        outputs = model(input_tensor)
        pred_idx = outputs.argmax(dim=1).item()
        pred_label = classes[pred_idx]

    # é¡¯ç¤ºçµæœèˆ‡ç•«æ¡†
    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)
    cv2.putText(frame, f'Prediction: {pred_label}', (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    cv2.imshow("Real-Time Garbage Classifier", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

