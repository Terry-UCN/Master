import os
import shutil
import random
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, datasets, models
from torch.utils.data import DataLoader
from tqdm import tqdm

# === 1. 資料切分 ===
def split_dataset(source_root, target_root, classes, test_ratio=0.2):
    for split in ['train', 'test']:
        for cls in classes:
            out_path = os.path.join(target_root, split, cls)
            os.makedirs(out_path, exist_ok=True)

    for cls in classes:
        src_folder = os.path.join(source_root, cls)
        all_images = [f for f in os.listdir(src_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        random.shuffle(all_images)

        n_total = len(all_images)
        n_test = int(n_total * test_ratio)

        test_images = all_images[:n_test]
        train_images = all_images[n_test:]

        for img in test_images:
            shutil.copy2(os.path.join(src_folder, img), os.path.join(target_root, 'test', cls, img))
        for img in train_images:
            shutil.copy2(os.path.join(src_folder, img), os.path.join(target_root, 'train', cls, img))

    print("✅ 資料切分完成。")


# === 2. 主程式 ===
def main():
    # === 資料與類別 ===
    source_root = r"C:\Users\CC\Desktop\tesy\data\Garbage classification\Garbage classification"
    target_root = r"C:\Users\CC\Desktop\tesy\data\Garbage classification split"
    classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic']
    num_classes = len(classes)

    # === 資料切分（第一次執行才執行）===
    if not os.path.exists(os.path.join(target_root, 'train')):
        split_dataset(source_root, target_root, classes)

    # === 圖像轉換（含資料增強） ===
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # EfficientNet 預訓練 mean/std
    ])
    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # === Dataset & DataLoader ===
    train_dataset = datasets.ImageFolder(root=os.path.join(target_root, 'train'), transform=transform_train)
    test_dataset = datasets.ImageFolder(root=os.path.join(target_root, 'test'), transform=transform_test)
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)

    # === EfficientNet 模型 ===
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = models.efficientnet_b0(pretrained=True)

    # 替換最後分類層
    in_features = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(in_features, num_classes)
    model = model.to(device)

    # === 損失與優化器 ===
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    # === 訓練模型 ===
    # === 訓練模型 ===
    best_acc = 0.0
    epochs = 10
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_acc = 100 * correct / total
        print(f"Train Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}%")

        # === 驗證模型 (每回合後驗證一次並儲存最佳模型) ===
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        val_acc = 100 * correct / total
        print(f"✅ Validation Accuracy: {val_acc:.2f}%")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), "best_trash_model.pth")
            print("💾 Best model saved!")

    print(f"\n🎉 最佳準確率: {best_acc:.2f}%")


if __name__ == "__main__":
    main()




import cv2
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image

# === 設定裝置 ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === 建立與載入模型 ===
model = models.efficientnet_b0(pretrained=False)
model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 5)
model.load_state_dict(torch.load("best_trash_model.pth", map_location=device))
model = model.to(device)
model.eval()

# === 類別名稱 ===
classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic']

# === 預處理流程（與訓練一致）===
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# === 開啟 webcam ===
cap = cv2.VideoCapture("CHANGE TO YOUR IP CAM ACCESS/video")
  # 替換為你的攝影機 URL

print("📷 開始即時辨識（按下 q 鍵可退出）")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # 擷取畫面中央區域作為目標框（可依實際調整）
    h, w, _ = frame.shape
    crop_size = 224
    center_x, center_y = w // 2, h // 2
    x1, y1 = center_x - crop_size // 2, center_y - crop_size // 2
    x2, y2 = center_x + crop_size // 2, center_y + crop_size // 2
    cropped = frame[y1:y2, x1:x2]

    # 預處理
    image = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))
    input_tensor = transform(image).unsqueeze(0).to(device)

    # 推論
    with torch.no_grad():
        outputs = model(input_tensor)
        pred_idx = outputs.argmax(dim=1).item()
        pred_label = classes[pred_idx]

    # 顯示結果與畫框
    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)
    cv2.putText(frame, f'Prediction: {pred_label}', (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    cv2.imshow("Real-Time Garbage Classifier", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

